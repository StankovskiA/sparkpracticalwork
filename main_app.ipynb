{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StringType, NumericType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, count, isnan, when, skewness, kurtosis, lit, percent_rank, udf, mean, concat, avg, format_string, concat_ws, hour, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session start and Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 02:18:29 WARN Utils: Your hostname, student-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/01/15 02:18:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/15 02:18:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DataLoadingExample\").getOrCreate()\n",
    "\n",
    "# Get the input data location from the command line or configuration\n",
    "input_data_location = \"data/1987.csv\"\n",
    "\n",
    "# Load the data into a PySpark DataFrame\n",
    "df = spark.read.csv(input_data_location, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Year' has only one unique value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'TailNum' has only one unique value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'TaxiOut' has only one unique value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'CancellationCode' has only one unique value\n",
      "Numerical Columns: ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'CRSArrTime', 'FlightNum', 'CRSElapsedTime', 'DepDelay', 'Distance', 'Cancelled']\n",
      "Categorical Columns: ['UniqueCarrier', 'Origin', 'Dest']\n"
     ]
    }
   ],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_remove = ['ArrTime', 'ActualElapsedTime', 'AirTime', 'TaxiIn', 'Diverted',\n",
    "                     'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay']\n",
    "\n",
    "\n",
    "# Remove columns with only one unique value\n",
    "for c in [c for c in df.columns if c not in columns_to_remove]:\n",
    "    if df.select(c).distinct().count() == 1:\n",
    "        print(\"Column '{}' has only one unique value\".format(c))\n",
    "        columns_to_remove.append(c)\n",
    "        \n",
    "# Select columns that are NOT in the 'columns_to_remove' list\n",
    "df = df.select([c for c in df.columns if c not in columns_to_remove])\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "categorical_cols = ['UniqueCarrier', 'Origin', 'Dest']\n",
    "numerical_cols = [x for x in df.columns if x not in categorical_cols]\n",
    "\n",
    "target_var = 'ArrDelay'\n",
    "\n",
    "# Remove the target variable 'ArrDelay' from the lists\n",
    "if target_var in numerical_cols:\n",
    "    numerical_cols.remove(target_var)\n",
    "if target_var in categorical_cols:\n",
    "    categorical_cols.remove(target_var)\n",
    "    \n",
    "# Print the lists\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|FlightNum|CRSElapsedTime|ArrDelay|DepDelay|Distance|Cancelled|UniqueCarrier_encoded|Origin_encoded|Dest_encoded|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "|   10|        14|        3|    741|       730|       849|     1451|            79|      23|      11|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        15|        4|    729|       730|       849|     1451|            79|      14|      -1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        17|        6|    741|       730|       849|     1451|            79|      29|      11|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        18|        7|    729|       730|       849|     1451|            79|      -2|      -1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        19|        1|    749|       730|       849|     1451|            79|      33|      19|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        21|        3|    728|       730|       849|     1451|            79|      -1|      -2|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        22|        4|    728|       730|       849|     1451|            79|       3|      -2|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        23|        5|    731|       730|       849|     1451|            79|      13|       1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        24|        6|    744|       730|       849|     1451|            79|      19|      14|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        25|        7|    729|       730|       849|     1451|            79|       2|      -1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        26|        1|    735|       730|       849|     1451|            79|      15|       5|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        28|        3|    741|       725|       855|     1451|            90|      24|      16|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        29|        4|    742|       725|       855|     1451|            90|      11|      17|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        31|        6|    726|       725|       855|     1451|            90|      -7|       1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|         1|        4|    936|       915|      1001|     1451|            46|      34|      21|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         2|        5|    918|       915|      1001|     1451|            46|      16|       3|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         3|        6|    928|       915|      1001|     1451|            46|      36|      13|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         4|        7|    914|       915|      1001|     1451|            46|       2|      -1|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         5|        1|   1042|       915|      1001|     1451|            46|      88|      87|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         6|        2|    934|       915|      1001|     1451|            46|      23|      19|     192|        0|                 11.0|           5.0|        62.0|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Label encode categorical columns\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_encoded\") for c in categorical_cols]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_encoded = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "df_encoded = df_encoded.drop(*categorical_cols)\n",
    "\n",
    "# Show the DataFrame with label-encoded categorical columns\n",
    "df_encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for numerical columns :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 02:19:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+--------------------+\n",
      "|summary|             Month|        DayofMonth|         DayOfWeek|           DepTime|       CRSDepTime|        CRSArrTime|        FlightNum|    CRSElapsedTime|          DepDelay|         Distance|           Cancelled|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+--------------------+\n",
      "|  count|           1311826|           1311826|           1311826|           1311826|          1311826|           1311826|          1311826|           1311826|           1311826|          1311826|             1311826|\n",
      "|   mean|10.993736211967136|15.717608890203426|3.9490427846375966|1369.2755101803905|1361.130906080532|1491.0518346183107|670.1427125243745|100.61908515306146| 8.063637791850889|591.2441015523978|0.015005801074227831|\n",
      "| stddev|0.8232011620414451| 8.835639995196434| 1.986289725601532| 478.7182790905976| 471.970102869255| 486.8002033250558|514.5509515450526| 61.23218933935588|24.024794200928753|498.1205841696614| 0.12157564836583548|\n",
      "|    min|                10|                 1|                 1|                 1|                1|                 1|                1|               -40|                -1|                0|                   0|\n",
      "|    max|                12|                31|                 7|                NA|             2359|              2400|             6282|              1555|                NA|               NA|                   1|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+--------------------+\n",
      "\n",
      "Frequency counts for UniqueCarrier_encoded :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+\n",
      "|UniqueCarrier_encoded| count|\n",
      "+---------------------+------+\n",
      "|                  0.0|185813|\n",
      "|                  1.0|165121|\n",
      "|                  2.0|152624|\n",
      "|                  3.0|123002|\n",
      "|                  4.0|116482|\n",
      "|                  5.0|108776|\n",
      "|                  6.0|108273|\n",
      "|                  7.0| 94814|\n",
      "|                  8.0| 69650|\n",
      "|                  9.0| 61975|\n",
      "|                 10.0| 45399|\n",
      "|                 11.0| 41706|\n",
      "|                 12.0| 21406|\n",
      "|                 13.0| 16785|\n",
      "+---------------------+------+\n",
      "\n",
      "Frequency counts for Origin_encoded :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Origin_encoded|count|\n",
      "+--------------+-----+\n",
      "|           0.0|67216|\n",
      "|           1.0|66309|\n",
      "|           2.0|51860|\n",
      "|           3.0|45646|\n",
      "|           4.0|43376|\n",
      "|           5.0|35155|\n",
      "|           6.0|32097|\n",
      "|           7.0|30991|\n",
      "|           8.0|29848|\n",
      "|           9.0|28765|\n",
      "|          10.0|28596|\n",
      "|          11.0|27548|\n",
      "|          12.0|25250|\n",
      "|          13.0|24518|\n",
      "|          14.0|23108|\n",
      "|          15.0|22016|\n",
      "|          16.0|21566|\n",
      "|          17.0|20570|\n",
      "|          18.0|19239|\n",
      "|          19.0|19081|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Frequency counts for Dest_encoded :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Dest_encoded|count|\n",
      "+------------+-----+\n",
      "|         0.0|67830|\n",
      "|         1.0|66783|\n",
      "|         2.0|52450|\n",
      "|         3.0|45597|\n",
      "|         4.0|44284|\n",
      "|         5.0|34948|\n",
      "|         6.0|32690|\n",
      "|         7.0|31058|\n",
      "|         8.0|30234|\n",
      "|         9.0|29296|\n",
      "|        10.0|28456|\n",
      "|        11.0|28452|\n",
      "|        12.0|25056|\n",
      "|        13.0|24664|\n",
      "|        14.0|23601|\n",
      "|        15.0|21979|\n",
      "|        16.0|21830|\n",
      "|        17.0|20474|\n",
      "|        18.0|19841|\n",
      "|        19.0|19149|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Missing values in each column:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 138:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|FlightNum|CRSElapsedTime|ArrDelay|DepDelay|Distance|Cancelled|UniqueCarrier_encoded|Origin_encoded|Dest_encoded|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "|    0|         0|        0|      0|         0|         0|        0|             0|       0|       0|       0|        0|                    0|             0|           0|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculating summary statistics for numerical columns\n",
    "print(\"Summary statistics for numerical columns :\")\n",
    "df_encoded.describe(numerical_cols).show()\n",
    "\n",
    "# Analyzing frequency counts for encoded categorical columns\n",
    "encoded_categorical_cols = [f\"{c}_encoded\" for c in categorical_cols]\n",
    "for col_name in encoded_categorical_cols:\n",
    "    print(f\"Frequency counts for {col_name} :\")\n",
    "    df_encoded.groupBy(col_name).count().orderBy('count', ascending=False).show()\n",
    "\n",
    "# Checking for missing values in each column\n",
    "print(\"Missing values in each column:\")\n",
    "df_encoded.select([count(when(col(c).isNull(), c)).alias(c) for c in df_encoded.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of Month: 0.011599187660133525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of DayofMonth: 0.019527508117766178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of DayOfWeek: 0.03900779848249456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of DepTime: -0.025141601840602965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of CRSDepTime: -0.0031534485545966925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of CRSArrTime: -0.2653019099069947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of FlightNum: 1.2344446971870777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of CRSElapsedTime: 1.8297136226970279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of DepDelay: 11.01423094278716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of Distance: 1.9919198527415156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of Cancelled: 7.978479083592512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of Month: -1.5242418620462543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of DayofMonth: -1.1909841994451253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of DayOfWeek: -1.2203884956540663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of DepTime: -0.8739366046995221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of CRSDepTime: -0.9126036848148988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of CRSArrTime: -0.509823562413986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of FlightNum: 1.7022216487506014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of CRSElapsedTime: 5.9880754659305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of DepDelay: 512.2954975172178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of Distance: 5.80610645337787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of Cancelled: 61.65612848736551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate skewness for numerical columns\n",
    "for c in numerical_cols:\n",
    "    skewness_value = df_encoded.select(skewness(c)).collect()[0][0]\n",
    "    print(f\"Skewness of {c}: {skewness_value}\")\n",
    "\n",
    "# Calculate kurtosis for numerical columns\n",
    "for c in numerical_cols:\n",
    "    kurtosis_value = df_encoded.select(kurtosis(c)).collect()[0][0]\n",
    "    print(f\"Kurtosis of {c}: {kurtosis_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skewness measures the asymmetry of the probability distribution of a real-valued random variable about its mean. If skewness is 0, the data are perfectly symmetrical, although it is quite unlikely for real-world data. A skewness value greater than 0 means that there is more weight in the left tail of the distribution.\n",
    "\n",
    "- Kurtosis indicates how the tails of a distribution differ from the tails of a normal distribution. High kurtosis means that the tails are fat, and there is a sharp peak (more outliers), whereas low kurtosis indicates light tails and a flat peak (less outlier-prone).\n",
    "\n",
    "From the results, we can see some variables with high skewness or kurtosis values, such as `Cancelled` and `Distance`, which may benefit from data transformation before being used in a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Month:\n",
      "    Lower bound: 7.0\n",
      "    Upper bound: 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column DayofMonth:\n",
      "    Lower bound: -14.5\n",
      "    Upper bound: 45.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column DayOfWeek:\n",
      "    Lower bound: -4.0\n",
      "    Upper bound: 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column CRSDepTime:\n",
      "    Lower bound: -263.0\n",
      "    Upper bound: 2945.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column CRSArrTime:\n",
      "    Lower bound: -80.5\n",
      "    Upper bound: 3107.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column FlightNum:\n",
      "    Lower bound: -613.5\n",
      "    Upper bound: 1798.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 55606\n",
      "Column CRSElapsedTime:\n",
      "    Lower bound: -42.0\n",
      "    Upper bound: 230.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 236:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 60988\n",
      "Column Cancelled:\n",
      "    Lower bound: 0.0\n",
      "    Upper bound: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 19685\n",
      "Column UniqueCarrier_encoded:\n",
      "    Lower bound: -8.0\n",
      "    Upper bound: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column Origin_encoded:\n",
      "    Lower bound: -51.0\n",
      "    Upper bound: 101.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 86721\n",
      "Column Dest_encoded:\n",
      "    Lower bound: -51.0\n",
      "    Upper bound: 101.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 248:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 85084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter out non-numerical columns based on the actual data type\n",
    "actual_numerical_cols = [f.name for f in df_encoded.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "# Define the bounds for the IQR\n",
    "bounds = {\n",
    "    c: dict(\n",
    "        zip([\"q1\", \"q3\"], df_encoded.approxQuantile(c, [0.25, 0.75], 0))\n",
    "    )\n",
    "    for c in actual_numerical_cols\n",
    "}\n",
    "\n",
    "for c in actual_numerical_cols:\n",
    "    iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "    lower_bound = bounds[c]['q1'] - (1.5 * iqr)\n",
    "    upper_bound = bounds[c]['q3'] + (1.5 * iqr)\n",
    "    \n",
    "    print(f\"Column {c}:\")\n",
    "    print(f\"    Lower bound: {lower_bound}\")\n",
    "    print(f\"    Upper bound: {upper_bound}\")\n",
    "    \n",
    "    # Optional: Filter out the outliers from the DataFrame\n",
    "    df_no_outliers = df_encoded.filter((col(c) >= lower_bound) & (col(c) <= upper_bound))\n",
    "    \n",
    "    # Optional: View the count of identified outliers\n",
    "    outliers_count = df_encoded.filter((col(c) < lower_bound) | (col(c) > upper_bound)).count()\n",
    "    print(f\"    Identified outliers: {outliers_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **No Outliers for Some Variables:** For columns like `Month`, `DayofMonth`, `DayOfWeek`, `CRSDepTime`, `CRSArrTime`, `UniqueCarrier_encoded`, and `Origin_encoded`, no outliers were identified based on the IQR method. This is expected for `Month`, `DayofMonth`, and `DayOfWeek` since their values fall within a specific range.\n",
    "\n",
    "2. **High Number of Outliers for Certain Variables:** For `FlightNum`, `CRSElapsedTime`, `Cancelled`, `Dest_encoded`, and a few others, a significant number of outliers have been identified. This could be due to large variances in flight numbers and destinations, or it might indicate data entry errors or actual unique cases.\n",
    "\n",
    "3. **Handling Outliers in Cancelled:** It's worth noting that `Cancelled` is a binary variable (likely indicating whether a flight was cancelled or not), so the concept of outliers based on IQR doesn't apply in the same way as it does to continuous data. If `Cancelled` has only two distinct values, 0 and 1, any \"outliers\" would be the cancelled flights themselves, and thus not outliers in the traditional sense.\n",
    "\n",
    "### Domain Knowledge\n",
    "\n",
    "1. **Cancelled Flights:** For the `Cancelled` variable, since these flights do not have a delay, they should be handled appropriately. One approach is to remove cancelled flights from the dataset when building a model to predict delays, as they don't contribute to the delay prediction.\n",
    "\n",
    "2. **FlightNum as Categorical Data:** Flight numbers represent specific flights and are categorical in nature, despite being encoded numerically. Each number may carry information about the route, frequency, or timing that could impact delays. Therefore, treat `FlightNum` as a categorical feature in the model, but be cautious of high cardinality which might require feature engineering techniques like embedding or binning.\n",
    "\n",
    "3. **CRSElapsedTime Extremes:** Extreme values in `CRSElapsedTime` represent long or short flights. These should not be considered outliers in a statistical sense but rather as part of the natural variability of flight durations. You may want to normalize or standardize these values for modeling purposes.\n",
    "\n",
    "4. **Origin and Destination Encoding:** Since the encoded values for `Origin` and `Destination` reflect actual airports, you should retain them as they are. These features can be quite informative for delay predictions as some airports might have higher delays due to factors like weather, traffic, or operational issues.\n",
    "\n",
    "5. **Domain Knowledge Integration:**\n",
    " - Consider the time of day and day of the week, as these can significantly impact flight delays.\n",
    " - Account for seasonal variations and potential weather impacts.\n",
    " - Include knowledge about airport congestion or known delay patterns at certain destinations or origins.\n",
    "\n",
    "6. **Modeling Considerations:** When building the model, consider the nature of each feature:\n",
    " - Use tree-based models if encoding of categorical features leads to high dimensionality.\n",
    " - For features with a wide range of values, consider binning or discretizing them.\n",
    " - Be mindful of overfitting, especially when dealing with features that have many unique categories.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of canceled flights before filtering: 19685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 254:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of canceled flights after filtering: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filterring canceled flight\n",
    "# Count the number of canceled flights before filtering\n",
    "cancelled_flights_count_before = df_encoded.filter(col('Cancelled') == 1).count()\n",
    "print(f\"Number of canceled flights before filtering: {cancelled_flights_count_before}\")\n",
    "\n",
    "# Remove canceled flights\n",
    "df_not_cancelled = df_encoded.filter(col('Cancelled') == 0)\n",
    "\n",
    "# Count the number of canceled flights after filtering\n",
    "cancelled_flights_count_after = df_not_cancelled.filter(col('Cancelled') == 1).count()\n",
    "print(f\"Number of canceled flights after filtering: {cancelled_flights_count_after}\")\n",
    "\n",
    "# Drop the 'Cancelled' column from the DataFrame\n",
    "df_encoded = df_not_cancelled.drop('Cancelled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 257:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+------+-----------------+\n",
      "|FlightDate|PartOfDay|IsWeekend|Season|DailyFlightVolume|\n",
      "+----------+---------+---------+------+-----------------+\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "|10-10     |Afternoon|1        |Autumn|13354            |\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "|10-10     |Evening  |1        |Autumn|13354            |\n",
      "|10-10     |Afternoon|1        |Autumn|13354            |\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "|10-10     |Evening  |1        |Autumn|13354            |\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "|10-10     |Afternoon|1        |Autumn|13354            |\n",
      "|10-10     |Afternoon|1        |Autumn|13354            |\n",
      "|10-10     |Afternoon|1        |Autumn|13354            |\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "|10-10     |Afternoon|1        |Autumn|13354            |\n",
      "|10-10     |Afternoon|1        |Autumn|13354            |\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "|10-10     |Afternoon|1        |Autumn|13354            |\n",
      "|10-10     |Morning  |1        |Autumn|13354            |\n",
      "+----------+---------+---------+------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# UDF to classify the time of day based on DepTime in hhmm format\n",
    "def get_part_of_day(deptime):\n",
    "    if deptime is None:\n",
    "        return None\n",
    "    hour = int(deptime) // 100\n",
    "    if 5 <= hour <= 11:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour <= 17:\n",
    "        return 'Afternoon'\n",
    "    elif 18 <= hour <= 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "part_of_day_udf = udf(get_part_of_day, StringType())\n",
    "\n",
    "# Now, let's recast the DepTime column to IntegerType to handle cases where it's not an integer.\n",
    "df_encoded = df_encoded.withColumn('DepTime', col('DepTime').cast(IntegerType()))\n",
    "\n",
    "# Then use the UDF to create the PartOfDay column\n",
    "df_encoded = df_encoded.withColumn('PartOfDay', part_of_day_udf(col('DepTime')))\n",
    "\n",
    "# Add IsWeekend column\n",
    "df_encoded = df_encoded.withColumn('IsWeekend', when(col('DayOfWeek').isin([6, 7]), 1).otherwise(0))\n",
    "\n",
    "\n",
    "\n",
    "# Add Season column based on the month\n",
    "df_encoded = df_encoded.withColumn('Season', when(col('Month').isin([12, 1, 2]), 'Winter')\n",
    "                                      .when(col('Month').isin([3, 4, 5]), 'Spring')\n",
    "                                      .when(col('Month').isin([6, 7, 8]), 'Summer')\n",
    "                                      .otherwise('Autumn'))\n",
    "\n",
    "# Create a 'FlightDate' key using 'Month' and 'DayofMonth' (as we only have data for 1987)\n",
    "df_encoded = df_encoded.withColumn('FlightDate', \n",
    "                                   concat_ws(\"-\", col('Month').cast(StringType()), col('DayofMonth').cast(StringType())))\n",
    "\n",
    "# Window for calculating the daily flight volume\n",
    "daily_volume_window = Window.partitionBy('FlightDate')\n",
    "\n",
    "# Add DailyFlightVolume column (counting the number of flights per day)\n",
    "df_encoded = df_encoded.withColumn('DailyFlightVolume', count('*').over(daily_volume_window))\n",
    "\n",
    "# Show the DataFrame with new features\n",
    "df_encoded.select('FlightDate', 'PartOfDay', 'IsWeekend', 'Season', 'DailyFlightVolume').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 266:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------------------+--------------+------------+---------+----------+-----------------+-------------+-------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|FlightNum|CRSElapsedTime|ArrDelay|DepDelay|Distance|UniqueCarrier_encoded|Origin_encoded|Dest_encoded|IsWeekend|FlightDate|DailyFlightVolume|SeasonVec    |PartOfDayVec |\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------------------+--------------+------------+---------+----------+-----------------+-------------+-------------+\n",
      "|10   |10        |6        |915    |915       |1001      |1451     |46            |21.0    |0.0     |192.0   |11.0                 |5.0           |62.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |10        |6        |1635   |1635      |1732      |1454     |57            |8.0     |0.0     |325.0   |11.0                 |42.0          |54.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |10        |6        |655    |650       |800       |1457     |70            |15.0    |5.0     |370.0   |11.0                 |8.0           |3.0         |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |10        |6        |930    |930       |1035      |1459     |65            |-3.0    |0.0     |337.0   |11.0                 |3.0           |5.0         |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |10        |6        |1119   |1120      |1250      |1459     |90            |12.0    |-1.0    |550.0   |11.0                 |5.0           |38.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |10        |6        |1828   |1830      |1929      |1460     |59            |2.0     |-2.0    |325.0   |11.0                 |42.0          |54.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[2],[1.0])|\n",
      "|10   |10        |6        |1759   |1800      |1901      |1463     |61            |-2.0    |-1.0    |325.0   |11.0                 |54.0          |42.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |10        |6        |829    |829       |946       |1472     |77            |10.0    |0.0     |446.0   |11.0                 |42.0          |29.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |10        |6        |1814   |1815      |1929      |1474     |74            |6.0     |-1.0    |446.0   |11.0                 |42.0          |29.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[2],[1.0])|\n",
      "|10   |10        |6        |1028   |1029      |1144      |1475     |75            |0.0     |-1.0    |446.0   |11.0                 |28.0          |42.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |10        |6        |1209   |1210      |1322      |1478     |72            |9.0     |-1.0    |446.0   |11.0                 |42.0          |29.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |10        |6        |1505   |1450      |1603      |1481     |73            |11.0    |15.0    |371.0   |11.0                 |48.0          |42.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |10        |6        |1648   |1648      |1806      |1485     |78            |-11.0   |0.0     |446.0   |11.0                 |28.0          |42.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |10        |6        |806    |800       |856       |1492     |56            |10.0    |6.0     |226.0   |11.0                 |48.0          |19.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |10        |6        |1332   |1310      |1423      |1494     |73            |27.0    |22.0    |371.0   |11.0                 |42.0          |48.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |10        |6        |1744   |1745      |1855      |1500     |70            |5.0     |-1.0    |373.0   |11.0                 |52.0          |3.0         |1        |10-10     |13354            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |10        |6        |850    |843       |1135      |1504     |112           |4.0     |7.0     |677.0   |11.0                 |3.0           |46.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |10        |6        |705    |700       |813       |1504     |73            |16.0    |5.0     |337.0   |11.0                 |5.0           |3.0         |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |10        |6        |1429   |1430      |1533      |1510     |63            |17.0    |-1.0    |326.0   |11.0                 |5.0           |54.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |10        |6        |710    |710       |801       |1512     |51            |4.0     |0.0     |236.0   |11.0                 |3.0           |19.0        |1        |10-10     |13354            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------------------+--------------+------------+---------+----------+-----------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert columns to the correct data types\n",
    "df_encoded = df_encoded.withColumn(\"DepDelay\", col(\"DepDelay\").cast(DoubleType()))\n",
    "df_encoded = df_encoded.withColumn(\"Distance\", col(\"Distance\").cast(DoubleType()))\n",
    "df_encoded = df_encoded.withColumn('ArrDelay', col('ArrDelay').cast(DoubleType()))\n",
    "\n",
    "# Define the columns to be indexed and encoded\n",
    "categorical_cols_to_encode = ['Season', 'PartOfDay']\n",
    "\n",
    "# Create a list to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# Iterate over the columns to create indexing and encoding stages\n",
    "for categorical_col in categorical_cols_to_encode:\n",
    "    # Create a StringIndexer\n",
    "    string_indexer = StringIndexer(inputCol=categorical_col, outputCol=categorical_col + \"Index\")\n",
    "    \n",
    "    # Create a OneHotEncoder\n",
    "    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[categorical_col + \"Vec\"])\n",
    "    \n",
    "    # Add the indexers and encoders to our pipeline stages\n",
    "    stages += [string_indexer, encoder]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline_model = pipeline.fit(df_encoded)\n",
    "\n",
    "# Transform the data\n",
    "df_encoded = pipeline_model.transform(df_encoded)\n",
    "\n",
    "# Now that we have our encoded features, we can remove the original categorical columns\n",
    "df_encoded = df_encoded.drop(*categorical_cols_to_encode)\n",
    "\n",
    "# And we can also remove the intermediate index columns\n",
    "for categorical_col in categorical_cols_to_encode:\n",
    "    df_encoded = df_encoded.drop(categorical_col + \"Index\")\n",
    "\n",
    "# Show the DataFrame with the new encoded columns\n",
    "df_encoded.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 269:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column ArrDelay has 3815 null values\n",
      "Column Distance has 994 null values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate a check for each column if there is any null value\n",
    "null_checks = [count(when(col(c).isNull(), c)).alias(c) for c in df_encoded.columns]\n",
    "\n",
    "# Apply the checks to the DataFrame\n",
    "null_counts = df_encoded.select(*null_checks).collect()[0].asDict()\n",
    "\n",
    "# Print out the counts of nulls for each column\n",
    "for column, null_count in null_counts.items():\n",
    "    if null_count != 0: \n",
    "        print(f\"Column {column} has {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 272:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines in the DataFrame is: 1292141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_lines = df_encoded.count()\n",
    "print(f\"The total number of lines in the DataFrame is: {total_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 275:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines after removing rows with null values in specified columns: 1287333\n",
      "\n",
      "Total number of deleted lines : 4808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# List of columns to check for null values\n",
    "columns_to_check = ['ArrDelay', 'Distance']\n",
    "\n",
    "# Drop rows that have null values in the specified columns\n",
    "df_encoded = df_encoded.na.drop(subset=columns_to_check)\n",
    "\n",
    "new_total_lines = df_encoded.count()\n",
    "\n",
    "# Show the number of lines remaining after removing rows with nulls in specific columns\n",
    "print(f\"The number of lines after removing rows with null values in specified columns: {new_total_lines}\")\n",
    "print(f\"\\nTotal number of deleted lines : {total_lines - new_total_lines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is now cleaned and processed, all ready to be use by a prediction model!\n",
    "\n",
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 02:23:19 WARN MemoryStore: Not enough space to cache rdd_676_1 in memory! (computed 72.4 MiB so far)\n",
      "24/01/15 02:23:19 WARN BlockManager: Persisting block rdd_676_1 to disk instead.\n",
      "24/01/15 02:23:21 WARN MemoryStore: Not enough space to cache rdd_676_0 in memory! (computed 72.4 MiB so far)\n",
      "24/01/15 02:23:21 WARN BlockManager: Persisting block rdd_676_0 to disk instead.\n",
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/student/.local/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "# Define the features and target variable\n",
    "feature_cols = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'CRSArrTime', 'FlightNum',\n",
    "                'CRSElapsedTime', 'DepDelay', 'Distance', 'UniqueCarrier_encoded', 'Origin_encoded',\n",
    "                'Dest_encoded', 'PartOfDayVec', 'IsWeekend', 'SeasonVec', 'DailyFlightVolume']\n",
    "\n",
    "target_col = 'ArrDelay'\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"  # Skip lines with null values\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_encoded)\n",
    "\n",
    "\n",
    "# Select only the features and target variable\n",
    "df_model_data = df_assembled.select(col(\"features\"), col(target_col).alias(\"label\"))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = df_model_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize the regression model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", maxBins=250)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "rf_predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def eval_model(predictions, metrics = ['rmse', 'mae', 'mse', 'r2']):\n",
    "    for m in metrics:\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=m)\n",
    "        metric = evaluator.evaluate(predictions)\n",
    "        print(f\" {m} on test data: {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rmse on test data: 15.194424250131549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mae on test data: 8.730181487756216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mse on test data: 230.8705282929857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 312:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r2 on test data: 0.6484485013307144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RandomForestRegressor evaluation\n",
    "eval_model(rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Root Mean Squared Error (RMSE):** 14.695933731417623\n",
    "- RMSE measures the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are.\n",
    "- A lower RMSE value indicates a better fit to the data. In your case, an RMSE of approximately 14.4 suggests that, on average, the model's predictions deviate from the actual values by about 14.4 units.\n",
    "\n",
    "2. **Mean Absolute Error (MAE):** 8.541987164922546\n",
    "- MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It's the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
    "- A lower MAE value is better. Your model's MAE of approximately 8.42 indicates that, on average, the absolute error of each prediction is around 8.42 units.\n",
    "\n",
    "3. **Mean Squared Error (MSE):** 215.97046823821833\n",
    "- MSE measures the average of the squares of the errorsthat is, the average squared difference between the estimated values and the actual value.\n",
    "- MSE is a risk metric corresponding to the expected value of the squared (quadratic) error or loss. The lower the MSE, the better the model. Your MSE is about 215.97, which aligns with the RMSE (since RMSE is the square root of MSE).\n",
    "\n",
    "4. **R-squared (R):** 0.6711371419348039\n",
    "- R is a statistical measure that represents the proportion of the variance for the dependent variable that's explained by the independent variables in a regression model.\n",
    "- R values range from 0 to 1, where higher values are generally better and indicate a higher proportion of variance accounted for by the model. Your R value of approximately 0.68 suggests that about 68% of the variance in your dependent variable is predictable from the independent variables.\n",
    "\n",
    "Overall, these metrics indicate a relatively good fit of the model to the data, though there's always room for improvement, especially in reducing the RMSE and MAE values. Additionally, the R value shows that a significant portion of the variance in the dataset is captured by your model. However, it's also important to consider the context of the specific dataset and problem domain when interpreting these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 02:24:51 WARN FileUtil: Failed to delete file or dir [/home/student/Documents/sparkpracticalwork/save/random_forest_model]: it still exists.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the trained rf model for later use if needed\n",
    "rf_model.write().overwrite().save(\"save/random_forest_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# RandomForestRegressor Hyperparameter tuning\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.maxBins, [250, 300, 350]) \\\n",
    "    .build()\n",
    "\n",
    "# CrossValidator\n",
    "crossval_rf = CrossValidator(estimator=rf,\n",
    "                             estimatorParamMaps=rf_paramGrid,\n",
    "                             evaluator=RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                             numFolds=3)\n",
    "\n",
    "# Fit the model using CrossValidator\n",
    "cv_rf_model = crossval_rf.fit(train_data)\n",
    "rf_best_model = cv_rf_model.bestModel\n",
    "best_rf_predictions = rf_best_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the best rf model\n",
    "eval_model(best_rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the tuned rf model for later use if needed\n",
    "rf_best_model.write().overwrite().save(\"save/best_random_forest_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 02:26:44 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/01/15 02:27:06 WARN DAGScheduler: Broadcasting large task binary with size 1059.4 KiB\n",
      "24/01/15 02:27:09 WARN DAGScheduler: Broadcasting large task binary with size 1757.9 KiB\n",
      "24/01/15 02:27:13 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/01/15 02:28:12 WARN DAGScheduler: Broadcasting large task binary with size 1072.1 KiB\n",
      "24/01/15 02:28:15 WARN DAGScheduler: Broadcasting large task binary with size 1769.4 KiB\n",
      "24/01/15 02:28:19 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/01/15 02:28:40 WARN DAGScheduler: Broadcasting large task binary with size 1069.6 KiB\n",
      "24/01/15 02:28:43 WARN DAGScheduler: Broadcasting large task binary with size 1791.3 KiB\n",
      "24/01/15 02:28:47 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/01/15 02:28:55 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "24/01/15 02:29:10 WARN DAGScheduler: Broadcasting large task binary with size 8.1 MiB\n",
      "24/01/15 02:29:30 WARN DAGScheduler: Broadcasting large task binary with size 1230.0 KiB\n",
      "24/01/15 02:29:40 WARN DAGScheduler: Broadcasting large task binary with size 9.1 MiB\n",
      "24/01/15 02:29:54 WARN DAGScheduler: Broadcasting large task binary with size 1244.9 KiB\n",
      "24/01/15 02:30:03 WARN DAGScheduler: Broadcasting large task binary with size 8.6 MiB\n",
      "24/01/15 02:30:18 WARN DAGScheduler: Broadcasting large task binary with size 1232.9 KiB\n",
      "24/01/15 02:30:26 WARN DAGScheduler: Broadcasting large task binary with size 9.3 MiB\n",
      "24/01/15 02:30:41 WARN DAGScheduler: Broadcasting large task binary with size 1240.3 KiB\n",
      "24/01/15 02:30:50 WARN DAGScheduler: Broadcasting large task binary with size 16.2 MiB\n",
      "24/01/15 02:30:55 ERROR Executor: Exception in task 0.0 in stage 568.0 (TID 613)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4979/0x0000000841a57040.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4943/0x0000000841a39040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3012/0x00000008412cc440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2732/0x00000008411abc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/01/15 02:30:55 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 568.0 (TID 613),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4979/0x0000000841a57040.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4943/0x0000000841a39040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3012/0x00000008412cc440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2732/0x00000008411abc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/01/15 02:30:55 WARN TaskSetManager: Lost task 0.0 in stage 568.0 (TID 613) (10.0.2.15 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4979/0x0000000841a57040.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4943/0x0000000841a39040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3012/0x00000008412cc440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2732/0x00000008411abc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/01/15 02:30:55 ERROR TaskSetManager: Task 0 in stage 568.0 failed 1 times; aborting job\n",
      "24/01/15 02:30:55 WARN TaskSetManager: Lost task 1.0 in stage 568.0 (TID 614) (10.0.2.15 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 568.0 failed 1 times, most recent failure: Lost task 0.0 in stage 568.0 (TID 613) (10.0.2.15 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4979/0x0000000841a57040.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4943/0x0000000841a39040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3012/0x00000008412cc440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2732/0x00000008411abc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/01/15 02:30:55 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 568.0 failed 1 times, most recent failure: Lost task 0.0 in stage 568.0 (TID 613) (10.0.2.15 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4979/0x0000000841a57040.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4943/0x0000000841a39040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3012/0x00000008412cc440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2732/0x00000008411abc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4979/0x0000000841a57040.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4943/0x0000000841a39040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3012/0x00000008412cc440.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2732/0x00000008411abc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "24/01/15 02:30:56 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:595)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:547)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3861)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3859)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/01/15 02:30:56 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:595)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:547)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3861)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3859)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/01/15 02:30:56 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:595)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:547)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3861)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3859)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 34304)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/student/.local/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# GBTRegressor for comparison\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\"; maxBins=250)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "predictions_gbt = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GBTRegressor predictions \n",
    "eval_model(predictions_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained gbt model for later use if needed\n",
    "gbt_model.write().overwrite().save(\"save/GBT_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close context\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
