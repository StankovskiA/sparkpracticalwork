{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StringType, NumericType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, count, isnan, when, skewness, kurtosis, lit, percent_rank, udf, mean, concat, avg, format_string, concat_ws, hour, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session start and Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/01/15 04:58:56 WARN Utils: Your hostname, PCAsusNathanG resolves to a loopback address: 127.0.1.1; using 172.20.198.4 instead (on interface eth0)\n",
      "24/01/15 04:58:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/15 04:58:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DataLoadingExample\").getOrCreate()\n",
    "\n",
    "# Get the input data location from the command line or configuration\n",
    "input_data_location = \"data/1987.csv\"\n",
    "\n",
    "# Load the data into a PySpark DataFrame\n",
    "df = spark.read.csv(input_data_location, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Year' has only one unique value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'TailNum' has only one unique value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'TaxiOut' has only one unique value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'CancellationCode' has only one unique value\n",
      "Numerical Columns: ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'CRSArrTime', 'FlightNum', 'CRSElapsedTime', 'DepDelay', 'Distance', 'Cancelled']\n",
      "Categorical Columns: ['UniqueCarrier', 'Origin', 'Dest']\n"
     ]
    }
   ],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_remove = ['ArrTime', 'ActualElapsedTime', 'AirTime', 'TaxiIn', 'Diverted', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay']\n",
    "\n",
    "\n",
    "# Remove columns with only one unique value\n",
    "for c in [c for c in df.columns if c not in columns_to_remove]:\n",
    "    if df.select(c).distinct().count() == 1:\n",
    "        print(\"Column '{}' has only one unique value\".format(c))\n",
    "        columns_to_remove.append(c)\n",
    "        \n",
    "# Select columns that are NOT in the 'columns_to_remove' list\n",
    "df = df.select([c for c in df.columns if c not in columns_to_remove])\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "categorical_cols = ['UniqueCarrier', 'Origin', 'Dest']\n",
    "numerical_cols = [x for x in df.columns if x not in categorical_cols]\n",
    "\n",
    "target_var = 'ArrDelay'\n",
    "\n",
    "# Remove the target variable 'ArrDelay' from the lists\n",
    "if target_var in numerical_cols:\n",
    "    numerical_cols.remove(target_var)\n",
    "if target_var in categorical_cols:\n",
    "    categorical_cols.remove(target_var)\n",
    "    \n",
    "# Print the lists\n",
    "print(\"Numerical Columns:\", numerical_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|FlightNum|CRSElapsedTime|ArrDelay|DepDelay|Distance|Cancelled|UniqueCarrier_encoded|Origin_encoded|Dest_encoded|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "|   10|        14|        3|    741|       730|       849|     1451|            79|      23|      11|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        15|        4|    729|       730|       849|     1451|            79|      14|      -1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        17|        6|    741|       730|       849|     1451|            79|      29|      11|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        18|        7|    729|       730|       849|     1451|            79|      -2|      -1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        19|        1|    749|       730|       849|     1451|            79|      33|      19|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        21|        3|    728|       730|       849|     1451|            79|      -1|      -2|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        22|        4|    728|       730|       849|     1451|            79|       3|      -2|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        23|        5|    731|       730|       849|     1451|            79|      13|       1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        24|        6|    744|       730|       849|     1451|            79|      19|      14|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        25|        7|    729|       730|       849|     1451|            79|       2|      -1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        26|        1|    735|       730|       849|     1451|            79|      15|       5|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        28|        3|    741|       725|       855|     1451|            90|      24|      16|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        29|        4|    742|       725|       855|     1451|            90|      11|      17|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|        31|        6|    726|       725|       855|     1451|            90|      -7|       1|     447|        0|                 11.0|          28.0|         5.0|\n",
      "|   10|         1|        4|    936|       915|      1001|     1451|            46|      34|      21|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         2|        5|    918|       915|      1001|     1451|            46|      16|       3|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         3|        6|    928|       915|      1001|     1451|            46|      36|      13|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         4|        7|    914|       915|      1001|     1451|            46|       2|      -1|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         5|        1|   1042|       915|      1001|     1451|            46|      88|      87|     192|        0|                 11.0|           5.0|        62.0|\n",
      "|   10|         6|        2|    934|       915|      1001|     1451|            46|      23|      19|     192|        0|                 11.0|           5.0|        62.0|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Label encode categorical columns\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_encoded\") for c in categorical_cols]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_encoded = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "df_encoded = df_encoded.drop(*categorical_cols)\n",
    "\n",
    "# Show the DataFrame with label-encoded categorical columns\n",
    "df_encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for numerical columns :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 05:01:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+--------------------+\n",
      "|summary|             Month|        DayofMonth|         DayOfWeek|           DepTime|        CRSDepTime|        CRSArrTime|        FlightNum|    CRSElapsedTime|          DepDelay|         Distance|           Cancelled|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+--------------------+\n",
      "|  count|           1311826|           1311826|           1311826|           1311826|           1311826|           1311826|          1311826|           1311826|           1311826|          1311826|             1311826|\n",
      "|   mean|10.993736211967136|15.717608890203426|3.9490427846375966|1369.2755101803905| 1361.130906080532|1491.0518346183107|670.1427125243745|100.61908515306146| 8.063637791850889|591.2441015523978|0.015005801074227831|\n",
      "| stddev|0.8232011620414477| 8.835639995196614|1.9862897256015322|478.71827909060164|471.97010286925854|486.80020332505114|514.5509515450487| 61.23218933935592|24.024794200929037|498.1205841696644|  0.1215756483658352|\n",
      "|    min|                10|                 1|                 1|                 1|                 1|                 1|                1|               -40|                -1|                0|                   0|\n",
      "|    max|                12|                31|                 7|                NA|              2359|              2400|             6282|              1555|                NA|               NA|                   1|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+--------------------+\n",
      "\n",
      "Frequency counts for UniqueCarrier_encoded :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+\n",
      "|UniqueCarrier_encoded| count|\n",
      "+---------------------+------+\n",
      "|                  0.0|185813|\n",
      "|                  1.0|165121|\n",
      "|                  2.0|152624|\n",
      "|                  3.0|123002|\n",
      "|                  4.0|116482|\n",
      "|                  5.0|108776|\n",
      "|                  6.0|108273|\n",
      "|                  7.0| 94814|\n",
      "|                  8.0| 69650|\n",
      "|                  9.0| 61975|\n",
      "|                 10.0| 45399|\n",
      "|                 11.0| 41706|\n",
      "|                 12.0| 21406|\n",
      "|                 13.0| 16785|\n",
      "+---------------------+------+\n",
      "\n",
      "Frequency counts for Origin_encoded :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Origin_encoded|count|\n",
      "+--------------+-----+\n",
      "|           0.0|67216|\n",
      "|           1.0|66309|\n",
      "|           2.0|51860|\n",
      "|           3.0|45646|\n",
      "|           4.0|43376|\n",
      "|           5.0|35155|\n",
      "|           6.0|32097|\n",
      "|           7.0|30991|\n",
      "|           8.0|29848|\n",
      "|           9.0|28765|\n",
      "|          10.0|28596|\n",
      "|          11.0|27548|\n",
      "|          12.0|25250|\n",
      "|          13.0|24518|\n",
      "|          14.0|23108|\n",
      "|          15.0|22016|\n",
      "|          16.0|21566|\n",
      "|          17.0|20570|\n",
      "|          18.0|19239|\n",
      "|          19.0|19081|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Frequency counts for Dest_encoded :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Dest_encoded|count|\n",
      "+------------+-----+\n",
      "|         0.0|67830|\n",
      "|         1.0|66783|\n",
      "|         2.0|52450|\n",
      "|         3.0|45597|\n",
      "|         4.0|44284|\n",
      "|         5.0|34948|\n",
      "|         6.0|32690|\n",
      "|         7.0|31058|\n",
      "|         8.0|30234|\n",
      "|         9.0|29296|\n",
      "|        10.0|28456|\n",
      "|        11.0|28452|\n",
      "|        12.0|25056|\n",
      "|        13.0|24664|\n",
      "|        14.0|23601|\n",
      "|        15.0|21979|\n",
      "|        16.0|21830|\n",
      "|        17.0|20474|\n",
      "|        18.0|19841|\n",
      "|        19.0|19149|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Missing values in each column:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 138:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|FlightNum|CRSElapsedTime|ArrDelay|DepDelay|Distance|Cancelled|UniqueCarrier_encoded|Origin_encoded|Dest_encoded|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "|    0|         0|        0|      0|         0|         0|        0|             0|       0|       0|       0|        0|                    0|             0|           0|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------+---------------------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculating summary statistics for numerical columns\n",
    "print(\"Summary statistics for numerical columns :\")\n",
    "df_encoded.describe(numerical_cols).show()\n",
    "\n",
    "# Analyzing frequency counts for encoded categorical columns\n",
    "encoded_categorical_cols = [f\"{c}_encoded\" for c in categorical_cols]\n",
    "for col_name in encoded_categorical_cols:\n",
    "    print(f\"Frequency counts for {col_name} :\")\n",
    "    df_encoded.groupBy(col_name).count().orderBy('count', ascending=False).show()\n",
    "\n",
    "# Checking for missing values in each column\n",
    "print(\"Missing values in each column:\")\n",
    "df_encoded.select([count(when(col(c).isNull(), c)).alias(c) for c in df_encoded.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of Month: 0.011599187660117735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of DayofMonth: 0.01952750811776666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of DayOfWeek: 0.039007798482500175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of DepTime: -0.025141601840601643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of CRSDepTime: -0.0031534485545970043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of CRSArrTime: -0.2653019099069996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of FlightNum: 1.2344446971871021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of CRSElapsedTime: 1.8297136226969979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of DepDelay: 11.014230942786806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of Distance: 1.9919198527415016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness of Cancelled: 7.978479083592336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of Month: -1.5242418620462475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of DayofMonth: -1.1909841994453017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of DayOfWeek: -1.2203884956540023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of DepTime: -0.8739366046995043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of CRSDepTime: -0.9126036848150698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of CRSArrTime: -0.5098235624138683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of FlightNum: 1.7022216487505473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of CRSElapsedTime: 5.988075465930576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of DepDelay: 512.295497518333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of Distance: 5.806106453377616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:==============>                                          (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis of Cancelled: 61.656128487320885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate skewness for numerical columns\n",
    "for c in numerical_cols:\n",
    "    skewness_value = df_encoded.select(skewness(c)).collect()[0][0]\n",
    "    print(f\"Skewness of {c}: {skewness_value}\")\n",
    "\n",
    "# Calculate kurtosis for numerical columns\n",
    "for c in numerical_cols:\n",
    "    kurtosis_value = df_encoded.select(kurtosis(c)).collect()[0][0]\n",
    "    print(f\"Kurtosis of {c}: {kurtosis_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skewness measures the asymmetry of the probability distribution of a real-valued random variable about its mean. If skewness is 0, the data are perfectly symmetrical, although it is quite unlikely for real-world data. A skewness value greater than 0 means that there is more weight in the left tail of the distribution.\n",
    "\n",
    "- Kurtosis indicates how the tails of a distribution differ from the tails of a normal distribution. High kurtosis means that the tails are fat, and there is a sharp peak (more outliers), whereas low kurtosis indicates light tails and a flat peak (less outlier-prone).\n",
    "\n",
    "From the results, we can see some variables with high skewness or kurtosis values, such as `Cancelled` and `Distance`, which may benefit from data transformation before being used in a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Month:\n",
      "    Lower bound: 7.0\n",
      "    Upper bound: 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column DayofMonth:\n",
      "    Lower bound: -14.5\n",
      "    Upper bound: 45.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column DayOfWeek:\n",
      "    Lower bound: -4.0\n",
      "    Upper bound: 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column CRSDepTime:\n",
      "    Lower bound: -263.0\n",
      "    Upper bound: 2945.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column CRSArrTime:\n",
      "    Lower bound: -80.5\n",
      "    Upper bound: 3107.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column FlightNum:\n",
      "    Lower bound: -613.5\n",
      "    Upper bound: 1798.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 55606\n",
      "Column CRSElapsedTime:\n",
      "    Lower bound: -42.0\n",
      "    Upper bound: 230.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 60988\n",
      "Column Cancelled:\n",
      "    Lower bound: 0.0\n",
      "    Upper bound: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 19685\n",
      "Column UniqueCarrier_encoded:\n",
      "    Lower bound: -8.0\n",
      "    Upper bound: 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 0\n",
      "Column Origin_encoded:\n",
      "    Lower bound: -51.0\n",
      "    Upper bound: 101.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 86721\n",
      "Column Dest_encoded:\n",
      "    Lower bound: -51.0\n",
      "    Upper bound: 101.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 259:==============>                                          (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Identified outliers: 85084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter out non-numerical columns based on the actual data type\n",
    "actual_numerical_cols = [f.name for f in df_encoded.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "# Define the bounds for the IQR\n",
    "bounds = {\n",
    "    c: dict(\n",
    "        zip([\"q1\", \"q3\"], df_encoded.approxQuantile(c, [0.25, 0.75], 0))\n",
    "    )\n",
    "    for c in actual_numerical_cols\n",
    "}\n",
    "\n",
    "for c in actual_numerical_cols:\n",
    "    iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "    lower_bound = bounds[c]['q1'] - (1.5 * iqr)\n",
    "    upper_bound = bounds[c]['q3'] + (1.5 * iqr)\n",
    "    \n",
    "    print(f\"Column {c}:\")\n",
    "    print(f\"    Lower bound: {lower_bound}\")\n",
    "    print(f\"    Upper bound: {upper_bound}\")\n",
    "    \n",
    "    # Optional: Filter out the outliers from the DataFrame\n",
    "    df_no_outliers = df_encoded.filter((col(c) >= lower_bound) & (col(c) <= upper_bound))\n",
    "    \n",
    "    # Optional: View the count of identified outliers\n",
    "    outliers_count = df_encoded.filter((col(c) < lower_bound) | (col(c) > upper_bound)).count()\n",
    "    print(f\"    Identified outliers: {outliers_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **No Outliers for Some Variables:** For columns like `Month`, `DayofMonth`, `DayOfWeek`, `CRSDepTime`, `CRSArrTime`, `UniqueCarrier_encoded`, no outliers were identified based on the IQR method. This is expected for `Month`, `DayofMonth`, and `DayOfWeek` since their values fall within a specific range.\n",
    "\n",
    "2. **High Number of Outliers for Certain Variables:** For `FlightNum`, `CRSElapsedTime`, `Cancelled`, `Origin_encoded` and `Dest_encoded`, a significant number of outliers have been identified.\n",
    "\n",
    "### Domain Knowledge\n",
    "\n",
    "1. **Cancelled Flights:** For the `Cancelled` variable, since these flights do not have a delay, they should be handled appropriately. One approach is to remove cancelled flights from the dataset when building a model to predict delays, as they don't contribute to the delay prediction.\n",
    "\n",
    "2. **CRSElapsedTime Extremes:** Extreme values in `CRSElapsedTime` represent long or short flights. These should not be considered outliers in a statistical sense but rather as part of the natural variability of flight durations.\n",
    "\n",
    "3. **Origin, Destination and Flight number:** Since the encoded values for `Origin_encoded` and `Dest_encoded` reflect actual airports and `FlightNum` give insights on specific flight lines, we'll retain them as they are. These features can be quite informative for delay predictions as some airports and specific flight might have usual higher delays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of canceled flights before filtering: 19685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 265:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of canceled flights after filtering: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filterring canceled flight\n",
    "# Count the number of canceled flights before filtering\n",
    "cancelled_flights_count_before = df_encoded.filter(col('Cancelled') == 1).count()\n",
    "print(f\"Number of canceled flights before filtering: {cancelled_flights_count_before}\")\n",
    "\n",
    "# Remove canceled flights\n",
    "df_not_cancelled = df_encoded.filter(col('Cancelled') == 0)\n",
    "\n",
    "# Count the number of canceled flights after filtering\n",
    "cancelled_flights_count_after = df_not_cancelled.filter(col('Cancelled') == 1).count()\n",
    "print(f\"Number of canceled flights after filtering: {cancelled_flights_count_after}\")\n",
    "\n",
    "# Drop the 'Cancelled' column from the DataFrame\n",
    "df_encoded = df_not_cancelled.drop('Cancelled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+------+-----------------+\n",
      "|FlightDate|PartOfDay|IsWeekend|Season|DailyFlightVolume|\n",
      "+----------+---------+---------+------+-----------------+\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Afternoon|1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Evening  |1        |Autumn|13351            |\n",
      "|10-24     |Evening  |1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Evening  |1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Afternoon|1        |Autumn|13351            |\n",
      "|10-24     |Afternoon|1        |Autumn|13351            |\n",
      "|10-24     |Afternoon|1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Morning  |1        |Autumn|13351            |\n",
      "|10-24     |Afternoon|1        |Autumn|13351            |\n",
      "|10-24     |Afternoon|1        |Autumn|13351            |\n",
      "+----------+---------+---------+------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF to classify the time of day based on DepTime in hhmm format\n",
    "def get_part_of_day(deptime):\n",
    "    if deptime is None:\n",
    "        return None\n",
    "    hour = int(deptime) // 100\n",
    "    if 5 <= hour <= 11:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour <= 17:\n",
    "        return 'Afternoon'\n",
    "    elif 18 <= hour <= 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "part_of_day_udf = udf(get_part_of_day, StringType())\n",
    "\n",
    "# Now, let's recast the DepTime column to IntegerType to handle cases where it's not an integer.\n",
    "df_encoded = df_encoded.withColumn('DepTime', col('DepTime').cast(IntegerType()))\n",
    "\n",
    "# Then use the UDF to create the PartOfDay column\n",
    "df_encoded = df_encoded.withColumn('PartOfDay', part_of_day_udf(col('DepTime')))\n",
    "\n",
    "# Add IsWeekend column\n",
    "df_encoded = df_encoded.withColumn('IsWeekend', when(col('DayOfWeek').isin([6, 7]), 1).otherwise(0))\n",
    "\n",
    "\n",
    "\n",
    "# Add Season column based on the month\n",
    "df_encoded = df_encoded.withColumn('Season', when(col('Month').isin([12, 1, 2]), 'Winter')\n",
    "                                      .when(col('Month').isin([3, 4, 5]), 'Spring')\n",
    "                                      .when(col('Month').isin([6, 7, 8]), 'Summer')\n",
    "                                      .otherwise('Autumn'))\n",
    "\n",
    "# Create a 'FlightDate' key using 'Month' and 'DayofMonth' (as we only have data for 1987)\n",
    "df_encoded = df_encoded.withColumn('FlightDate', \n",
    "                                   concat_ws(\"-\", col('Month').cast(StringType()), col('DayofMonth').cast(StringType())))\n",
    "\n",
    "# Window for calculating the daily flight volume\n",
    "daily_volume_window = Window.partitionBy('FlightDate')\n",
    "\n",
    "# Add DailyFlightVolume column (counting the number of flights per day)\n",
    "df_encoded = df_encoded.withColumn('DailyFlightVolume', count('*').over(daily_volume_window))\n",
    "\n",
    "# Show the DataFrame with new features\n",
    "df_encoded.select('FlightDate', 'PartOfDay', 'IsWeekend', 'Season', 'DailyFlightVolume').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 279:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------------------+--------------+------------+---------+----------+-----------------+-------------+-------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|FlightNum|CRSElapsedTime|ArrDelay|DepDelay|Distance|UniqueCarrier_encoded|Origin_encoded|Dest_encoded|IsWeekend|FlightDate|DailyFlightVolume|SeasonVec    |PartOfDayVec |\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------------------+--------------+------------+---------+----------+-----------------+-------------+-------------+\n",
      "|10   |24        |6        |744    |730       |849       |1451     |79            |19.0    |14.0    |447.0   |11.0                 |28.0          |5.0         |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |929    |915       |1001      |1451     |46            |51.0    |14.0    |192.0   |11.0                 |5.0           |62.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |1634   |1635      |1732      |1454     |57            |13.0    |-1.0    |325.0   |11.0                 |42.0          |54.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |24        |6        |652    |650       |800       |1457     |70            |8.0     |2.0     |370.0   |11.0                 |8.0           |3.0         |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |931    |930       |1035      |1459     |65            |7.0     |1.0     |337.0   |11.0                 |3.0           |5.0         |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |1120   |1120      |1250      |1459     |90            |13.0    |0.0     |550.0   |11.0                 |5.0           |38.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |1829   |1830      |1929      |1460     |59            |-1.0    |-1.0    |325.0   |11.0                 |42.0          |54.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[2],[1.0])|\n",
      "|10   |24        |6        |1801   |1800      |1901      |1463     |61            |5.0     |1.0     |325.0   |11.0                 |54.0          |42.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[2],[1.0])|\n",
      "|10   |24        |6        |836    |829       |946       |1472     |77            |7.0     |7.0     |446.0   |11.0                 |42.0          |29.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |1815   |1815      |1929      |1474     |74            |9.0     |0.0     |446.0   |11.0                 |42.0          |29.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[2],[1.0])|\n",
      "|10   |24        |6        |1028   |1029      |1144      |1475     |75            |1.0     |-1.0    |446.0   |11.0                 |28.0          |42.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |1210   |1210      |1322      |1478     |72            |6.0     |0.0     |446.0   |11.0                 |42.0          |29.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |24        |6        |1448   |1450      |1603      |1481     |73            |0.0     |-2.0    |371.0   |11.0                 |48.0          |42.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |24        |6        |1649   |1648      |1806      |1485     |78            |0.0     |1.0     |446.0   |11.0                 |28.0          |42.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |24        |6        |814    |815       |928       |1489     |73            |-2.0    |-1.0    |371.0   |11.0                 |48.0          |42.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |952    |953       |1106      |1490     |73            |8.0     |-1.0    |371.0   |11.0                 |42.0          |48.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |1134   |1129      |1225      |1490     |56            |17.0    |5.0     |226.0   |11.0                 |48.0          |19.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |759    |800       |856       |1492     |56            |-1.0    |-1.0    |226.0   |11.0                 |48.0          |19.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[1],[1.0])|\n",
      "|10   |24        |6        |1309   |1310      |1423      |1494     |73            |-4.0    |-1.0    |371.0   |11.0                 |42.0          |48.0        |1        |10-24     |13351            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "|10   |24        |6        |1745   |1745      |1855      |1500     |70            |3.0     |0.0     |373.0   |11.0                 |52.0          |3.0         |1        |10-24     |13351            |(1,[0],[1.0])|(3,[0],[1.0])|\n",
      "+-----+----------+---------+-------+----------+----------+---------+--------------+--------+--------+--------+---------------------+--------------+------------+---------+----------+-----------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert columns to the correct data types\n",
    "df_encoded = df_encoded.withColumn(\"DepDelay\", col(\"DepDelay\").cast(DoubleType()))\n",
    "df_encoded = df_encoded.withColumn(\"Distance\", col(\"Distance\").cast(DoubleType()))\n",
    "df_encoded = df_encoded.withColumn('ArrDelay', col('ArrDelay').cast(DoubleType()))\n",
    "\n",
    "# Define the columns to be indexed and encoded\n",
    "categorical_cols_to_encode = ['Season', 'PartOfDay']\n",
    "\n",
    "# Create a list to hold the stages of the pipeline\n",
    "stages = []\n",
    "\n",
    "# Iterate over the columns to create indexing and encoding stages\n",
    "for categorical_col in categorical_cols_to_encode:\n",
    "    # Create a StringIndexer\n",
    "    string_indexer = StringIndexer(inputCol=categorical_col, outputCol=categorical_col + \"Index\")\n",
    "    \n",
    "    # Create a OneHotEncoder\n",
    "    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[categorical_col + \"Vec\"])\n",
    "    \n",
    "    # Add the indexers and encoders to our pipeline stages\n",
    "    stages += [string_indexer, encoder]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline_model = pipeline.fit(df_encoded)\n",
    "\n",
    "# Transform the data\n",
    "df_encoded = pipeline_model.transform(df_encoded)\n",
    "\n",
    "# Now that we have our encoded features, we can remove the original categorical columns\n",
    "df_encoded = df_encoded.drop(*categorical_cols_to_encode)\n",
    "\n",
    "# And we can also remove the intermediate index columns\n",
    "for categorical_col in categorical_cols_to_encode:\n",
    "    df_encoded = df_encoded.drop(categorical_col + \"Index\")\n",
    "\n",
    "# Show the DataFrame with the new encoded columns\n",
    "df_encoded.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column ArrDelay has 3815 null values\n",
      "Column Distance has 994 null values\n"
     ]
    }
   ],
   "source": [
    "# Generate a check for each column if there is any null value\n",
    "null_checks = [count(when(col(c).isNull(), c)).alias(c) for c in df_encoded.columns]\n",
    "\n",
    "# Apply the checks to the DataFrame\n",
    "null_counts = df_encoded.select(*null_checks).collect()[0].asDict()\n",
    "\n",
    "# Print out the counts of nulls for each column\n",
    "for column, null_count in null_counts.items():\n",
    "    if null_count != 0: \n",
    "        print(f\"Column {column} has {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines in the DataFrame is: 1292141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_lines = df_encoded.count()\n",
    "print(f\"The total number of lines in the DataFrame is: {total_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 286:=================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines after removing rows with null values in specified columns: 1287333\n",
      "\n",
      "Total number of deleted lines : 4808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# List of columns to check for null values\n",
    "columns_to_check = ['ArrDelay', 'Distance']\n",
    "\n",
    "# Drop rows that have null values in the specified columns\n",
    "df_encoded = df_encoded.na.drop(subset=columns_to_check)\n",
    "\n",
    "new_total_lines = df_encoded.count()\n",
    "\n",
    "# Show the number of lines remaining after removing rows with nulls in specific columns\n",
    "print(f\"The number of lines after removing rows with null values in specified columns: {new_total_lines}\")\n",
    "print(f\"\\nTotal number of deleted lines : {total_lines - new_total_lines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is now cleaned and processed, all ready to be use by a prediction model!\n",
    "\n",
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 05:07:45 WARN MemoryStore: Not enough space to cache rdd_709_3 in memory! (computed 13.7 MiB so far)\n",
      "24/01/15 05:07:45 WARN MemoryStore: Not enough space to cache rdd_709_7 in memory! (computed 9.0 MiB so far)\n",
      "24/01/15 05:07:45 WARN MemoryStore: Not enough space to cache rdd_709_4 in memory! (computed 13.7 MiB so far)\n",
      "24/01/15 05:07:45 WARN BlockManager: Persisting block rdd_709_7 to disk instead.\n",
      "24/01/15 05:07:45 WARN BlockManager: Persisting block rdd_709_4 to disk instead.\n",
      "24/01/15 05:07:45 WARN BlockManager: Persisting block rdd_709_3 to disk instead.\n",
      "24/01/15 05:07:45 WARN MemoryStore: Not enough space to cache rdd_709_0 in memory! (computed 13.7 MiB so far)\n",
      "24/01/15 05:07:45 WARN BlockManager: Persisting block rdd_709_0 to disk instead.\n",
      "24/01/15 05:07:45 WARN MemoryStore: Not enough space to cache rdd_709_5 in memory! (computed 13.7 MiB so far)\n",
      "24/01/15 05:07:45 WARN BlockManager: Persisting block rdd_709_5 to disk instead.\n",
      "24/01/15 05:07:45 WARN MemoryStore: Not enough space to cache rdd_709_2 in memory! (computed 13.7 MiB so far)\n",
      "24/01/15 05:07:45 WARN BlockManager: Persisting block rdd_709_2 to disk instead.\n",
      "24/01/15 05:07:45 WARN MemoryStore: Not enough space to cache rdd_709_6 in memory! (computed 20.5 MiB so far)\n",
      "24/01/15 05:07:45 WARN BlockManager: Persisting block rdd_709_6 to disk instead.\n",
      "24/01/15 05:07:45 WARN MemoryStore: Not enough space to cache rdd_709_1 in memory! (computed 20.5 MiB so far)\n",
      "24/01/15 05:07:45 WARN BlockManager: Persisting block rdd_709_1 to disk instead.\n",
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "# Define the features and target variable\n",
    "feature_cols = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'CRSArrTime', 'FlightNum',\n",
    "                'CRSElapsedTime', 'DepDelay', 'Distance', 'UniqueCarrier_encoded', 'Origin_encoded',\n",
    "                'Dest_encoded', 'PartOfDayVec', 'IsWeekend', 'SeasonVec', 'DailyFlightVolume']\n",
    "\n",
    "target_col = 'ArrDelay'\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"  # Skip lines with null values\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_encoded)\n",
    "\n",
    "\n",
    "# Select only the features and target variable\n",
    "df_model_data = df_assembled.select(col(\"features\"), col(target_col).alias(\"label\"))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = df_model_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize the regression model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", maxBins=250)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "rf_predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def eval_model(predictions, metrics = ['rmse', 'mae', 'mse', 'r2']):\n",
    "    for m in metrics:\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=m)\n",
    "        metric = evaluator.evaluate(predictions)\n",
    "        print(f\" {m} on test data: {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rmse on test data: 16.1142797385221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mae on test data: 8.771058928398741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mse on test data: 259.6700114913439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 326:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r2 on test data: 0.6278799780508133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RandomForestRegressor evaluation\n",
    "eval_model(rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Root Mean Squared Error (RMSE):** Standard deviation of the residuals (prediction errors : lower RMSE value = better fit to the data).\n",
    "Here : Model's predictions deviate from about 16 minutes from the actual delay time.\n",
    "\n",
    "2. **Mean Absolute Error (MAE):** Average magnitude of the errors in a set of predictions, without considering their direction (Average of the absolute differences between prediction and actual observation where all individual differences have equal weight : lower MAE = better).\n",
    "RF model's MAE of almost 9 indicates that, on average, the absolute error of each prediction is around 9 minutes.\n",
    "\n",
    "3. **Mean Squared Error (MSE):** Average of the squares of the errorsthat is, the average squared difference between the estimated values and the actual value (Risk metric corresponding to the expected value of the squared (quadratic) error or loss : Low MSE = good model).\n",
    "RF model's MSE is about 260, which aligns with the RMSE (since RMSE is the square root of MSE).\n",
    "\n",
    "4. **R-squared (R):** Statistical measure that represents the proportion of the variance for the dependent variable that's explained by the independent variables in a regression model (Value range from 0 to 1 : High value = higher proportion of variance accounted for by the model). \n",
    "RF model's R value of approximately 0.62 suggests that about 62% of the variance in your dependent variable is predictable from the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunning and Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 05:13:31 WARN DAGScheduler: Broadcasting large task binary with size 1086.0 KiB\n",
      "24/01/15 05:13:45 WARN DAGScheduler: Broadcasting large task binary with size 1087.6 KiB\n",
      "24/01/15 05:13:58 WARN DAGScheduler: Broadcasting large task binary with size 1076.0 KiB\n",
      "24/01/15 05:14:13 WARN DAGScheduler: Broadcasting large task binary with size 1086.0 KiB\n",
      "24/01/15 05:14:15 WARN DAGScheduler: Broadcasting large task binary with size 1825.3 KiB\n",
      "24/01/15 05:14:19 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "24/01/15 05:14:26 ERROR Executor: Exception in task 4.0 in stage 546.0 (TID 2136)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/01/15 05:14:26 ERROR Executor: Exception in task 1.0 in stage 546.0 (TID 2133)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5883/0x0000000841a47840.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5815/0x0000000841a1d040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2983/0x00000008412be840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2704/0x000000084119a440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/01/15 05:14:26 ERROR Executor: Exception in task 2.0 in stage 546.0 (TID 2134)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/01/15 05:14:26 ERROR Executor: Exception in task 0.0 in stage 546.0 (TID 2132)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5883/0x0000000841a47840.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5815/0x0000000841a1d040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2983/0x00000008412be840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2704/0x000000084119a440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/01/15 05:14:26 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 546.0 (TID 2136),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/01/15 05:14:26 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 546.0 (TID 2134),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/01/15 05:14:26 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 546.0 (TID 2133),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5883/0x0000000841a47840.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5815/0x0000000841a1d040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2983/0x00000008412be840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2704/0x000000084119a440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/01/15 05:14:26 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 546.0 (TID 2132),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5883/0x0000000841a47840.apply(Unknown Source)\n",
      "\tat scala.Array$.tabulate(Array.scala:418)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5815/0x0000000841a1d040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2983/0x00000008412be840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2704/0x000000084119a440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/01/15 05:14:26 WARN TaskSetManager: Lost task 4.0 in stage 546.0 (TID 2136) (172.20.198.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "24/01/15 05:14:26 ERROR TaskSetManager: Task 4 in stage 546.0 failed 1 times; aborting job\n",
      "24/01/15 05:14:26 WARN TaskSetManager: Lost task 5.0 in stage 546.0 (TID 2137) (172.20.198.4 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 546.0 failed 1 times, most recent failure: Lost task 4.0 in stage 546.0 (TID 2136) (172.20.198.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/01/15 05:14:26 WARN TaskSetManager: Lost task 7.0 in stage 546.0 (TID 2139) (172.20.198.4 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 546.0 failed 1 times, most recent failure: Lost task 4.0 in stage 546.0 (TID 2136) (172.20.198.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/01/15 05:14:26 WARN TaskSetManager: Lost task 6.0 in stage 546.0 (TID 2138) (172.20.198.4 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 546.0 failed 1 times, most recent failure: Lost task 4.0 in stage 546.0 (TID 2136) (172.20.198.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/01/15 05:14:26 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 546.0 failed 1 times, most recent failure: Lost task 4.0 in stage 546.0 (TID 2136) (172.20.198.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "24/01/15 05:14:26 WARN TaskSetManager: Lost task 3.0 in stage 546.0 (TID 2135) (172.20.198.4 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 546.0 failed 1 times, most recent failure: Lost task 4.0 in stage 546.0 (TID 2136) (172.20.198.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "----------------------------------------                            (0 + 1) / 9]\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39496)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_14532/552518764.py\", line 15, in <module>\n",
      "    cv_rf_model = crossval_rf.fit(train_data)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/tuning.py\", line 847, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 873, in next\n",
      "    raise value\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/tuning.py\", line 847, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Fit the model using CrossValidator\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m cv_rf_model \u001b[38;5;241m=\u001b[39m \u001b[43mcrossval_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m rf_best_model \u001b[38;5;241m=\u001b[39m cv_rf_model\u001b[38;5;241m.\u001b[39mbestModel\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# RandomForestRegressor Hyperparameter tuning\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 15, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 8, 12]) \\\n",
    "    .addGrid(rf.maxBins, [250, 300, 350]) \\\n",
    "    .build()\n",
    "\n",
    "# CrossValidator\n",
    "crossval_rf = CrossValidator(estimator=rf,\n",
    "                             estimatorParamMaps=rf_paramGrid,\n",
    "                             evaluator=RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                             numFolds=3)\n",
    "\n",
    "# Fit the model using CrossValidator\n",
    "cv_rf_model = crossval_rf.fit(train_data)\n",
    "rf_best_model = cv_rf_model.bestModel\n",
    "best_rf_predictions = rf_best_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best rf model\n",
    "eval_model(best_rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning requires to much computational power and our personal machine and the VM provided by the course are not enough to run the code. We will therefore skip this part for now.\n",
    "\n",
    "### GBT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# GBTRegressor for comparison\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m gbt \u001b[38;5;241m=\u001b[39m \u001b[43mGBTRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxBins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m gbt_model \u001b[38;5;241m=\u001b[39m gbt\u001b[38;5;241m.\u001b[39mfit(train_data)\n\u001b[1;32m      4\u001b[0m predictions_gbt \u001b[38;5;241m=\u001b[39m gbt_model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/regression.py:1793\u001b[0m, in \u001b[0;36mGBTRegressor.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, subsamplingRate, checkpointInterval, lossType, maxIter, stepSize, seed, impurity, featureSubsetStrategy, validationTol, validationIndicatorCol, leafCol, minWeightFractionPerNode, weightCol)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;124;03m__init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;124;03m         maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;124;03m         weightCol=None)\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28msuper\u001b[39m(GBTRegressor, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m-> 1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.ml.regression.GBTRegressor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/pyspark/ml/wrapper.py:84\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjava_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_obj(\u001b[38;5;241m*\u001b[39mjava_args)\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/mnt/c/Users/natha/Documents/N7/Ecole/3A/S9_etranger/Cours/Lundi/Big_Data/Practical_work/sparkpracticalwork/spark_venv/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# GBTRegressor for comparison\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\", maxBins=250)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "predictions_gbt = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GBTRegressor predictions \n",
    "eval_model(predictions_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
